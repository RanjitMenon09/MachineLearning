{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33533f8e",
   "metadata": {},
   "source": [
    "  ## <div align=\"center\">Labsheet-05-Ranjit-Menon </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe60b8b",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "#### Implement TF-IDF algorithm using 'textblob' package. Use the  files  given in the folder. The output should be the TF-IDF value of each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1694f8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install necessary package\n",
    "#!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16d2321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob as tb \n",
    "import math\n",
    "import os\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf022a40",
   "metadata": {},
   "source": [
    "### Step 1 - loading file data into Dataset\n",
    "**prepare_dataset()** \n",
    "This method is responsible for loading all the files under Lab5_Data and iterate through each file and load the content into pandas dataframe, this dataframe will be used in other places to load the data.\n",
    "\n",
    "It contains two column, document - name of the file and content - the content for each file, we will be mostly using content from this dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7574d027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to load the dataset with file content\n",
    "def prepare_dataset() :\n",
    "    folder_path = 'Lab5_Data' #path to folder\n",
    "\n",
    "    # list of all files in the folder\n",
    "    file_list = os.listdir(folder_path)\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    # Loop through each file, read its content, and create a DataFrame\n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Open and read the content of the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        # Create a DataFrame for the current file\n",
    "        current_df = pd.DataFrame({'document': [file_name], 'content': [content]})\n",
    "\n",
    "        # Append the DataFrame to the list as I am getting depcrecation error when appending directly in dataframe\n",
    "        dfs.append(current_df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    df = pd.concat(dfs, ignore_index=True)    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32aaefaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pearl1.txt</td>\n",
       "      <td>\\n \\n\\nJohn Steinbeck\\n\\n  \\t\\t\\t\\n\\t\\n\\n    \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pearl2.txt</td>\n",
       "      <td>\\n\\nJohn Steinbeck\\nChapter 2\\nJohn Steinbeck\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pearl3.txt</td>\n",
       "      <td>\\n\\nJohn Steinbeck\\nChapter 3\\nJohn Steinbeck\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pearl4.txt</td>\n",
       "      <td>\\n\\nJohn Steinbeck\\nChapter 4\\nJohn Steinbeck\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pearl5.txt</td>\n",
       "      <td>\\n\\nJohn Steinbeck\\nChapter 5\\nJohn Steinbeck\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pearl6.txt</td>\n",
       "      <td>\\n\\nJohn Steinbeck\\nChapter 6\\nJohn Steinbeck\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     document                                            content\n",
       "0  Pearl1.txt  \\n \\n\\nJohn Steinbeck\\n\\n  \\t\\t\\t\\n\\t\\n\\n    \"...\n",
       "1  Pearl2.txt  \\n\\nJohn Steinbeck\\nChapter 2\\nJohn Steinbeck\\...\n",
       "2  Pearl3.txt  \\n\\nJohn Steinbeck\\nChapter 3\\nJohn Steinbeck\\...\n",
       "3  Pearl4.txt  \\n\\nJohn Steinbeck\\nChapter 4\\nJohn Steinbeck\\...\n",
       "4  Pearl5.txt  \\n\\nJohn Steinbeck\\nChapter 5\\nJohn Steinbeck\\...\n",
       "5  Pearl6.txt  \\n\\nJohn Steinbeck\\nChapter 6\\nJohn Steinbeck\\..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#call dataset prepation method\n",
    "df = prepare_dataset()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1140dbf7",
   "metadata": {},
   "source": [
    "### Step 2 - Cleaning up the corpus by removing stop words.\n",
    "**remove_stopwords**\n",
    "This method will remove the stop words from the sentence, advantage of removing this is to get more relevant words for tf-idf calculation and also it will keep the corpus length small as this words does not have contribute in the weightage of the words.\n",
    "We are using **nltk library** to remove the stop words.\n",
    "\n",
    "Stopwords typically include words from various parts of speech, such as:\n",
    "\n",
    "**Articles:** e.g., \"a,\" \"an,\" \"the\"\n",
    "\n",
    "**Conjunctions:** e.g., \"and,\" \"but,\" \"or\"\n",
    "\n",
    "**Prepositions:** e.g., \"in,\" \"on,\" \"at,\" \"with\"\n",
    "\n",
    "**Pronouns:** e.g., \"I,\" \"you,\" \"he,\" \"she,\" \"it,\" \"we,\" \"they\"\n",
    "\n",
    "**Auxiliary verbs:** e.g., \"is,\" \"am,\" \"are,\" \"was,\" \"were,\" \"be,\" \"been,\" \"have,\" \"has,\" \"had,\" \"do,\" \"does,\" \"did\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a102d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     document                                            content\n",
      "0  Pearl1.txt  John Steinbeck \"In town tell story great pearl...\n",
      "1  Pearl2.txt  John Steinbeck Chapter 2 John Steinbeck town l...\n",
      "2  Pearl3.txt  John Steinbeck Chapter 3 John Steinbeck town t...\n",
      "3  Pearl4.txt  John Steinbeck Chapter 4 John Steinbeck wonder...\n",
      "4  Pearl5.txt  John Steinbeck Chapter 5 John Steinbeck late m...\n",
      "5  Pearl6.txt  John Steinbeck Chapter 6 John Steinbeck wind b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ranjit09\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to remove stop words from text\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #print(stop_words)\n",
    "    filtered_words = [word for word in text.split() if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Iterate through the DataFrame and update 'content' column\n",
    "for index, row in df.iterrows():\n",
    "    df.at[index, 'content'] = remove_stopwords(row['content'])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2caaeb1",
   "metadata": {},
   "source": [
    "### Step 3 - Cleaning up the corpus by removing punctuation\n",
    "**remove_punctuation**\n",
    "This method will remove the punctuation from the sentence, this will keep the corpus clean by having only the words which can be used for tf-idf calculation, excluding puncutation we can prioritize meaningful words, avoid noise and improve text analysis accuracy.\n",
    "\n",
    "example of punctuation : !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f915fee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     document                                            content\n",
      "0  Pearl1.txt  john steinbeck in town tell story great pearl ...\n",
      "1  Pearl2.txt  john steinbeck chapter 2 john steinbeck town l...\n",
      "2  Pearl3.txt  john steinbeck chapter 3 john steinbeck town t...\n",
      "3  Pearl4.txt  john steinbeck chapter 4 john steinbeck wonder...\n",
      "4  Pearl5.txt  john steinbeck chapter 5 john steinbeck late m...\n",
      "5  Pearl6.txt  john steinbeck chapter 6 john steinbeck wind b...\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation(text):    \n",
    "    blob = tb(text) #use TextBlob to convert this into document/corpus\n",
    "    words_without_punct = [word for word in blob.words if word not in string.punctuation]\n",
    "    text_without_punct = ' '.join(words_without_punct)\n",
    "    return text_without_punct\n",
    "\n",
    "# Iterate through the DataFrame and update 'content' column\n",
    "for index, row in df.iterrows():\n",
    "    df.at[index, 'content'] = remove_punctuation(row['content'])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f2873",
   "metadata": {},
   "source": [
    "### Step 4 - Helper method for tf-idf calculation\n",
    "**tf**\n",
    "The method calculates term frequency (TF) by dividing the count of a word in a text blob by its length.\n",
    "\n",
    "![Alt Text](image/tf.png)\n",
    "\n",
    "**idf**\n",
    "The method calculates Inverse Document Frequency (IDF) by taking the logarithm of the ratio of the total number of documents to the number containing a specific word.\n",
    "\n",
    "![Alt Text](image/idf.png)\n",
    "\n",
    "**tfidf**\n",
    "The method computes the Term Frequency-Inverse Document Frequency (TF-IDF) score for a word in a document by multiplying the term frequence (tf) by Inverse Document Frequency (idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a25d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "#this method returns 1 if a word exists in the document and 0 if it does not.\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36de375d",
   "metadata": {},
   "source": [
    "### Step 5 - Calculating the TF-IDF score - Without lowercase\n",
    "The below code snippet will get all the content from the dataframe and iterate through each document and calculate the tf-idf score followed by it sort the score in descending order for the scores dictionary tuple and output the top 5 with highest score for each document\n",
    "\n",
    "#### if you look below the word Scorpion and scoripion are repeated with different score, the same is the case for trackers and Trackers, this change the way the score is assigned as it considers this word differently, but below we will do one more run by lower caseing the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad43ba9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in document 1\n",
      "\tWord: Scorpion, TF-IDF: 0.0065\n",
      "\tWord: doctor, TF-IDF: 0.0048\n",
      "\tWord: scorpion, TF-IDF: 0.0041\n",
      "\tWord: tail, TF-IDF: 0.00295\n",
      "\tWord: Enemy, TF-IDF: 0.00295\n",
      "Top words in document 2\n",
      "\tWord: oysters, TF-IDF: 0.01054\n",
      "\tWord: oyster, TF-IDF: 0.00958\n",
      "\tWord: basket, TF-IDF: 0.00862\n",
      "\tWord: Might, TF-IDF: 0.00766\n",
      "\tWord: shell, TF-IDF: 0.00544\n",
      "Top words in document 3\n",
      "\tWord: Pearl, TF-IDF: 0.00678\n",
      "\tWord: doctor, TF-IDF: 0.00448\n",
      "\tWord: News, TF-IDF: 0.00368\n",
      "\tWord: Come, TF-IDF: 0.00232\n",
      "\tWord: priest, TF-IDF: 0.00221\n",
      "Top words in document 4\n",
      "\tWord: Pearl, TF-IDF: 0.0099\n",
      "\tWord: One, TF-IDF: 0.00767\n",
      "\tWord: dealer, TF-IDF: 0.00587\n",
      "\tWord: Another, TF-IDF: 0.00419\n",
      "\tWord: Let, TF-IDF: 0.00344\n",
      "Top words in document 5\n",
      "\tWord: god, TF-IDF: 0.00301\n",
      "\tWord: bring, TF-IDF: 0.00301\n",
      "\tWord: Quietly, TF-IDF: 0.00226\n",
      "\tWord: pathway, TF-IDF: 0.00226\n",
      "\tWord: boat, TF-IDF: 0.00222\n",
      "Top words in document 6\n",
      "\tWord: Little, TF-IDF: 0.01928\n",
      "\tWord: trackers, TF-IDF: 0.00601\n",
      "\tWord: Trackers, TF-IDF: 0.00601\n",
      "\tWord: pool, TF-IDF: 0.00474\n",
      "\tWord: Two, TF-IDF: 0.00379\n"
     ]
    }
   ],
   "source": [
    "# Create TextBlob objects and add them to the bloblist\n",
    "bloblist = [tb(content) for content in df['content']]\n",
    "\n",
    "for i, blob in enumerate(bloblist):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    #print(scores.items())\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:5]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0741b735",
   "metadata": {},
   "source": [
    "\n",
    "### Step 6 - Calculating the TF-IDF score - With lowercase\n",
    "The below code snippet will get all the content from the dataframe and iterate through each document and calculate the tf-idf score followed by it sort the score in descending order for the scores dictionary tuple and output the top 5 with highest score for each document\n",
    "\n",
    "#### Now let us see how this changes when we make the text as lower case, as the above one consider the same words in different case seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e79a693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in document 1\n",
      "\tWord: doctor, TF-IDF: 0.0048\n",
      "\tWord: scorpion, TF-IDF: 0.0041\n",
      "\tWord: tail, TF-IDF: 0.00295\n",
      "\tWord: hanging, TF-IDF: 0.00261\n",
      "\tWord: rope, TF-IDF: 0.00236\n",
      "Top words in document 2\n",
      "\tWord: oysters, TF-IDF: 0.01054\n",
      "\tWord: oyster, TF-IDF: 0.00958\n",
      "\tWord: basket, TF-IDF: 0.00862\n",
      "\tWord: shell, TF-IDF: 0.00544\n",
      "\tWord: shells, TF-IDF: 0.00479\n",
      "Top words in document 3\n",
      "\tWord: doctor, TF-IDF: 0.00448\n",
      "\tWord: priest, TF-IDF: 0.00221\n",
      "\tWord: school, TF-IDF: 0.00184\n",
      "\tWord: books, TF-IDF: 0.00184\n",
      "\tWord: capsule, TF-IDF: 0.00184\n",
      "Top words in document 4\n",
      "\tWord: dealer, TF-IDF: 0.00587\n",
      "\tWord: buyer, TF-IDF: 0.00335\n",
      "\tWord: coin, TF-IDF: 0.00335\n",
      "\tWord: crowd, TF-IDF: 0.00293\n",
      "\tWord: offer, TF-IDF: 0.00251\n",
      "Top words in document 5\n",
      "\tWord: bring, TF-IDF: 0.00301\n",
      "\tWord: pathway, TF-IDF: 0.00226\n",
      "\tWord: boat, TF-IDF: 0.00222\n",
      "\tWord: moon, TF-IDF: 0.00194\n",
      "\tWord: wind, TF-IDF: 0.00194\n",
      "Top words in document 6\n",
      "\tWord: trackers, TF-IDF: 0.00601\n",
      "\tWord: pool, TF-IDF: 0.00474\n",
      "\tWord: cave, TF-IDF: 0.00348\n",
      "\tWord: watcher, TF-IDF: 0.00348\n",
      "\tWord: cleft, TF-IDF: 0.00316\n"
     ]
    }
   ],
   "source": [
    "df['content'] = df['content'].str.lower() #convert the data into lowercase\n",
    "\n",
    "bloblist = [tb(content) for content in df['content']]\n",
    "\n",
    "for i, blob in enumerate(bloblist):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    #print(scores.items())\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:5]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65086c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
