{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup nltk and all data and package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ranjit.x.menon\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: autocorrect in c:\\users\\ranjit.x.menon\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: pySpellChecker in c:\\users\\ranjit.x.menon\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ranjit.x.menon\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ranjit.x.menon\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\ranjit.x.menon\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\ranjit.x.menon\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\ranjit.x.menon\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk autocorrect pySpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from autocorrect import Speller\n",
    "from spellchecker import SpellChecker\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ranjit.x.menon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'know', 'not', 'what', 'cause', 'others', 'may', 'take', ',', 'but', 'as', 'for', 'me', ',', 'give', 'me', 'libery', 'or', 'give', 'me', 'death', '!']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I know not what cause others may take, but as for me, give me libery or give me death!\"\n",
    "word_tokens = word_tokenize(sentence)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am xyz.', 'I stay in India.', 'I am doing good']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I am xyz. I stay in India. I am doing good\"\n",
    "sentence_tokens = sent_tokenize(sentence)\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell check using Autocorrect library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London\n",
      "Today iss a bad day\n"
     ]
    }
   ],
   "source": [
    "spellCorrect = Speller(lang=\"en\")\n",
    "print(spellCorrect(\"Lodon\"))\n",
    "print(spellCorrect(\"Tooday iss a bads day\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell check using PyspellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "london\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['today', 'iss', 'a', 'bads', 'day']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellCorrect = SpellChecker()\n",
    "print(spellCorrect.correction(\"Lodon\"))\n",
    "words = spellCorrect.split_words(\"Tooday iss a bads day\")\n",
    "[spellCorrect.correction(word) for word in words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tooday', \"'s\", 'a', 'grat', 'dya']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['is', 'great', 'today', 'day']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellCorrect = SpellChecker()\n",
    "words = word_tokenize(\"Tooday's a grat dya\")\n",
    "print(words)\n",
    "unknownWords = spellCorrect.unknown(words)\n",
    "[spellCorrect.correction(word) for word in unknownWords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ranjit.x.menon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens ['We', 'had', 'a', 'Wonderfull', 'party', 'yesterday', ',', 'in', 'the', 'house', 'of', 'my', 'friend', ',', 'He', 'stays', 'in', 'Bangalore', '.']\n",
      "Filtered Tokens ['Wonderfull', 'party', 'yesterday', ',', 'house', 'friend', ',', 'stays', 'Bangalore', '.']\n"
     ]
    }
   ],
   "source": [
    "list_stopwords = stopwords.words(\"english\")\n",
    "#print(set_stopwords)\n",
    "sentence = \"We had a Wonderfull party yesterday, in the house of my friend, He stays in Bangalore.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "print(\"Tokens\", tokens)\n",
    "filterd_sentence = [w for w in tokens if not w.lower() in list_stopwords]\n",
    "print(\"Filtered Tokens\", filterd_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning by removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['it', \"'s\", 'a', 'great', 'Day', 'is', \"n't\", 'it', 'Do', 'you', 'get', 'what', 'I', 'am', 'saying', 'AH']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(punctuation)\n",
    "text = \"it's a great Day!, isn't it, Do you get what I am saying? AH!! . . . / /\"\n",
    "tokens = word_tokenize(text)\n",
    "#print(w_tokens)\n",
    "finalText = [w for w in tokens if not w in punctuation]\n",
    "print(finalText)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f9605aacf38f8e88d0f218a5d74a0c056495fade7d8bc51608c5911ef056993"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
